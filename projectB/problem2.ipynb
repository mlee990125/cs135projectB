{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVDpp\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import accuracy\n",
    "from train_valid_test_loader import load_train_valid_test_datasets\n",
    "\n",
    "#======================================================\n",
    "# Step 0: Load data\n",
    "#======================================================\n",
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()\n",
    "\n",
    "user_id_train, item_id_train, rating_train = train_tuple\n",
    "user_id_valid, item_id_valid, rating_valid = valid_tuple\n",
    "# user_id_test, item_id_test, rating_test = test_tuple  # Might not be needed here, but available if you want to measure test.\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'user_id': user_id_train,\n",
    "    'item_id': item_id_train,\n",
    "    'rating': rating_train\n",
    "})\n",
    "\n",
    "valid_df = pd.DataFrame({\n",
    "    'user_id': user_id_valid,\n",
    "    'item_id': item_id_valid,\n",
    "    'rating': rating_valid\n",
    "})\n",
    "\n",
    "# Adjust rating scale as needed (1 to 5 if it's MovieLens)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "train_data = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "valid_data = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "validset = valid_data.build_full_trainset()\n",
    "validset_list = validset.build_testset()  # Convert valid to testset for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#======================================================\n",
    "# Step 1: Hyperparameter Tuning with SVD++\n",
    "#======================================================\n",
    "param_grid = {\n",
    "    'n_factors': [20, 50, 100],\n",
    "    'lr_all': [0.002, 0.005],\n",
    "    'reg_all': [0.02, 0.1],\n",
    "    'n_epochs': [20, 50]\n",
    "}\n",
    "\n",
    "# Using GridSearchCV with SVD++\n",
    "gs = GridSearchCV(SVDpp, param_grid, measures=['mae'], cv=3, n_jobs=-1)\n",
    "gs.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract results from GridSearchCV in Surprise\n",
    "results = [\n",
    "    {\n",
    "        'params': params,\n",
    "        'mean_test_mae': mean_mae\n",
    "    }\n",
    "    for params, mean_mae in zip(gs.cv_results['params'], gs.cv_results['mean_test_mae'])\n",
    "]\n",
    "\n",
    "# Plot hyperparameter selection\n",
    "n_factors = [res['params']['n_factors'] for res in results]\n",
    "mae_scores = [res['mean_test_mae'] for res in results]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_factors, mae_scores, marker='o', label='Validation MAE')\n",
    "plt.xlabel('Number of Latent Factors (n_factors)', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "plt.title('Hyperparameter Selection: Validation MAE vs. n_factors', fontsize=14)\n",
    "plt.xticks(n_factors)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best MAE score with SVD++:\", gs.best_score['mae'])\n",
    "print(\"Best params with SVD++:\", gs.best_params['mae'])\n",
    "\n",
    "best_params = gs.best_params['mae']\n",
    "algo = SVDpp(**best_params)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Evaluate on validation set\n",
    "valid_preds = algo.test(validset_list)\n",
    "valid_mae = accuracy.mae(valid_preds, verbose=True)\n",
    "print(f\"Validation MAE with chosen SVD++: {valid_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#======================================================\n",
    "# Step 2: Combine train+valid for final model training\n",
    "#======================================================\n",
    "combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "combined_data = Dataset.load_from_df(combined_df[['user_id', 'item_id', 'rating']], reader)\n",
    "combined_trainset = combined_data.build_full_trainset()\n",
    "\n",
    "final_algo = SVDpp(**best_params)\n",
    "final_algo.fit(combined_trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#======================================================\n",
    "# Step 3: Predict on leaderboard dataset\n",
    "#======================================================\n",
    "leaderboard_df = pd.read_csv('./data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "leaderboard_user_ids = leaderboard_df['user_id'].values\n",
    "leaderboard_item_ids = leaderboard_df['item_id'].values\n",
    "\n",
    "testset_leaderboard = [(u, i, 0.0) for u, i in zip(leaderboard_user_ids, leaderboard_item_ids)]\n",
    "leaderboard_preds = final_algo.test(testset_leaderboard)\n",
    "predicted_ratings = np.array([pred.est for pred in leaderboard_preds])\n",
    "\n",
    "# Save the predictions\n",
    "np.savetxt('predicted_ratings_leaderboard2.txt', predicted_ratings, fmt='%.4f')\n",
    "print(\"Predictions saved to predicted_ratings_leaderboard2.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#======================================================\n",
    "# Step 3: Predict on leaderboard dataset\n",
    "#======================================================\n",
    "leaderboard_df = pd.read_csv('./data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "leaderboard_user_ids = leaderboard_df['user_id'].values\n",
    "leaderboard_item_ids = leaderboard_df['item_id'].values\n",
    "\n",
    "testset_leaderboard = [(u, i, 0.0) for u, i in zip(leaderboard_user_ids, leaderboard_item_ids)]\n",
    "leaderboard_preds = final_algo.test(testset_leaderboard)\n",
    "predicted_ratings = np.array([pred.est for pred in leaderboard_preds])\n",
    "\n",
    "# Save the predictions\n",
    "np.savetxt('predicted_ratings_leaderboard2.txt', predicted_ratings, fmt='%.4f')\n",
    "print(\"Predictions saved to predicted_ratings_leaderboard2.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
